ğŸ¦Š KURAMA

Enterprise-Grade Hybrid AI Assistant
Built with DevOps principles. Designed for intelligence, reliability, and control.

Inspired by the Nine-Tailed Fox â€” powerful, controlled, and engineered.

ğŸ§  Overview

KURAMA is a modular hybrid AI assistant framework designed with production-grade architecture and DevOps best practices.

It combines:

Local-first AI processing using Ollama
Secure system command orchestration
REST API interface (FastAPI)
Containerized deployment
CI/CD-ready architecture
Extensible cloud intelligence (future support for providers like OpenAI)

KURAMA is not just a chatbot â€” it is an AI system engineered with infrastructure discipline.

ğŸ— Architecture

KURAMA follows a modular layered architecture:

User Input
    â†“
Intent Classification Layer
    â†“
Decision Engine (Routing)
   â†™                     â†˜
Local LLM             Command Executor
(Ollama)              (Validated & Secure)
    â†“
Response Formatter
    â†“
API Output

Design Principles

ğŸ” Security-first command validation
ğŸ§  Intent-driven routing logic
âš™ï¸ Modular service separation
ğŸ“¦ Containerized runtime
ğŸ“Š Observability-ready structure
â˜ï¸ Hybrid-ready (Local + Cloud extensibility)


ğŸ“ Project Structure

kurama/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ chakra_engine.py     # Routing logic (formerly brain)
â”‚   â”œâ”€â”€ sharingan.py         # Intent classifier
â”‚   â””â”€â”€ memory.py
â”‚
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ ollama_client.py
â”‚
â”œâ”€â”€ commands/
â”‚   â”œâ”€â”€ executor.py
â”‚   â””â”€â”€ allowed_commands.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py
â”‚
â””â”€â”€ .github/workflows/
        ci.yml
```bash
kurama/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ chakra_engine.py
â”‚   â”œâ”€â”€ sharingan.py
â”‚   â””â”€â”€ memory.py
â”‚
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ ollama_client.py
â”‚
â”œâ”€â”€ commands/
â”‚   â”œâ”€â”€ executor.py
â”‚   â””â”€â”€ allowed_commands.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py
â”‚
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ ci.yml

âš™ï¸ Core Capabilities (Phase 1)

Local LLM inference via Ollama
Secure system command execution layer
FastAPI REST interface
Dockerized deployment
CI pipeline integration
Structured modular backend
